{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T00:14:31.401579100Z",
     "start_time": "2024-09-05T00:14:29.878694600Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import shap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T00:14:38.400167700Z",
     "start_time": "2024-09-05T00:14:38.391161Z"
    }
   },
   "outputs": [],
   "source": [
    "dataStreamPath = os.getcwd() + \"\\\\\"\n",
    "graphsStreamPath= os.getcwd() + \"\\\\graphs\\\\\"\n",
    "shapvaluesStreamPath= os.getcwd() + \"\\\\shapvalues\\\\\"\n",
    "oneheadmodelsStreamPath= os.getcwd() + \"\\\\onehead_models\\\\\"\n",
    "baselinemodelsStreamPath= os.getcwd() + \"\\\\baseline_models\\\\\"\n",
    "protoformsStreamPath= os.getcwd() + \"\\\\protoforms_small\\\\\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T00:14:42.691986300Z",
     "start_time": "2024-09-05T00:14:42.334665900Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"df_train_noise.csv\")\n",
    "df_test = pd.read_csv(\"df_test_noise.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T00:14:48.484378100Z",
     "start_time": "2024-09-05T00:14:48.433594Z"
    }
   },
   "outputs": [],
   "source": [
    "#TRAIN SET\n",
    "X_train = df_train.loc[:, 'pcm_LOGenergy_sma':'pcm_fftMag_mfcc_12_']\n",
    "y_train_symptoms = df_train.loc[:, 'anxiety':'suicide']\n",
    "y_train_states = df_train.loc[:, 'hamd_ymrs']\n",
    "\n",
    "# from categorical to numeric target\n",
    "label_coding = {'euthymia' : 0,\n",
    "                'depression' : 1,\n",
    "                'mania' : 2,\n",
    "                'mixed': 3}\n",
    "\n",
    "y_train_states_encoded = np.array(y_train_states.map(label_coding).astype(int))\n",
    "\n",
    "# #TEST SET\n",
    "X_test = df_test.loc[:, 'pcm_LOGenergy_sma':'pcm_fftMag_mfcc_12_']\n",
    "y_test_symptoms = df_test.loc[:, 'anxiety':'suicide']\n",
    "y_test_states = df_test.loc[:, 'hamd_ymrs']\n",
    "\n",
    "y_test_states_encoded = np.array(y_test_states.map(label_coding).astype(int))\n",
    "\n",
    "# # standardize data\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(X_train.values)\n",
    "X_train_scaled = scaler.transform(X_train.values)\n",
    "X_test_scaled = scaler.transform(X_test.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-04T23:43:59.355191500Z"
    }
   },
   "outputs": [],
   "source": [
    "classes_names_states= list(label_coding.keys())\n",
    "\n",
    "feature_names=X_test.columns\n",
    "\n",
    "X_train_scale_df = pd.DataFrame(X_train_scaled, columns = feature_names)\n",
    "\n",
    "#Train the XGBoost model\n",
    "#We create a dictionary that contains our model hyperparameters\n",
    "xgb_params = {\n",
    "    'n_estimators': 500, \n",
    "    #'learning_rate': 0.1,\n",
    "    #'subsample': 0.8,\n",
    "    #'reg_alpha': 1,\n",
    "    'max_depth': 3, #it was 10\n",
    "    'objective': 'multi:softprob', #'binary:logistic',\n",
    "    'num_class': 4\n",
    "    #'scale_pos_weight': 5\n",
    "}\n",
    "xgb_model = XGBClassifier(**xgb_params,use_label_encoder =False)\n",
    "xgb_model = xgb_model.fit(X_train_scale_df, y_train_states_encoded) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-04T23:43:59.360192600Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred_xgb=xgb_model.predict(X_test)\n",
    "xgb_cm = confusion_matrix(y_test_states_encoded, y_pred_xgb, labels=xgb_model.classes_)\n",
    "xgb_cr = classification_report(y_test_states_encoded, y_pred_xgb)\n",
    "\n",
    "\n",
    "print(xgb_cm)\n",
    "print(xgb_cr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Baseline approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.a) Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-04T23:43:59.376197400Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dense(64, input_shape=(86,), activation='relu', name='dense'),\n",
    "        tf.keras.layers.Dropout(0.2, name='dropout'),\n",
    "        tf.keras.layers.Dense(4, activation='softmax', name='output')])\n",
    "    model.build()\n",
    "    return model\n",
    "\n",
    "model_name = \"baseline\"\n",
    "baseline = build_model()\n",
    "\n",
    "baseline.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-04T23:43:59.385198100Z"
    }
   },
   "outputs": [],
   "source": [
    "baseline.compile(optimizer='adam',\n",
    "                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), \n",
    "                metrics=['accuracy'])\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3) \n",
    "\n",
    "baseline.fit(X_train_scaled, y_train_states_encoded, epochs=15,\n",
    "            validation_data=(X_test_scaled, y_test_states_encoded),\n",
    "            callbacks=[early_stopping])\n",
    "\n",
    "y_pred_states = baseline.predict(X_test_scaled)\n",
    "y_pred_states = np.argmax(y_pred_states, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-04T23:43:59.389437300Z"
    }
   },
   "outputs": [],
   "source": [
    "cm_base = confusion_matrix(y_test_states_encoded, y_pred_states)\n",
    "cr_base = classification_report(y_test_states_encoded, y_pred_states)\n",
    "\n",
    "print(cm_base)\n",
    "print(cr_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.b) SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T23:43:59.405609400Z",
     "start_time": "2024-09-04T23:43:59.393497600Z"
    }
   },
   "outputs": [],
   "source": [
    "#prepare data for shap\n",
    "X_train_summary = shap.sample(X_train_scaled, 100)\n",
    "end = len(X_test_scaled)\n",
    "feature_names=X_test.columns \n",
    "classes_names_states= list(label_coding.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-04T23:43:59.405609400Z"
    }
   },
   "outputs": [],
   "source": [
    "#calculate shap values\n",
    "explainer = shap.KernelExplainer(baseline.predict, X_train_summary) \n",
    "shap_values = explainer.shap_values(X_test_scaled[1:end:100, : ]) \n",
    "data_shap_base = pd.DataFrame(X_test_scaled[1:end:100,:], columns = feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-04T23:43:59.420614900Z"
    }
   },
   "outputs": [],
   "source": [
    "# PLOTS\n",
    "for max_features in [20, 40, 86]: \n",
    "  shap.summary_plot(shap_values, X_test_scaled[1:end:10,:], plot_type=\"bar\", class_names= classes_names_states,\n",
    "                    feature_names = feature_names, max_display=max_features , show=False)\n",
    "  plt.gcf()\n",
    "  figname=graphsStreamPath+model_name+'_global_allclasses_states_'+str(max_features)+'.png'\n",
    "  plt.savefig(figname,dpi=150, bbox_inches='tight')\n",
    "  plt.clf()\n",
    "\n",
    "#I'm plotting the global explanations for all the classes, varying the number of features to show\n",
    "#I'm iterating on the number of classes (numerical)\n",
    "for class_id in range(len(shap_values)):\n",
    "  #I'm iterating on the number of features I want to plot\n",
    "  for max_features in [20, 40, 86]: \n",
    "    shap.summary_plot(shap_values[class_id], X_test_scaled[1:end:100,:], feature_names = feature_names,\n",
    "                      max_display=max_features,show=False)\n",
    "    plt.gcf()\n",
    "    figname=graphsStreamPath+model_name+'_global_class'+str(class_id)+'_features'+ str(max_features)+'.png'\n",
    "    plt.savefig(figname,dpi=150, bbox_inches='tight')\n",
    "    plt.clf()\n",
    "\n",
    "shap_values_0_class_base = pd.DataFrame(shap_values[0], columns = feature_names)\n",
    "shap_values_1_class_base = pd.DataFrame(shap_values[1], columns = feature_names)\n",
    "shap_values_2_class_base = pd.DataFrame(shap_values[2], columns = feature_names)\n",
    "shap_values_3_class_base = pd.DataFrame(shap_values[3], columns = feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Compositional MLP approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.a) Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-04T23:43:59.428165100Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "model_name='one_head'\n",
    "\n",
    "input = tf.keras.layers.Input(shape=(86,), name='input')\n",
    "hidden = tf.keras.layers.Dense(64, activation='relu', name='dense')(input)\n",
    "dropout = tf.keras.layers.Dropout(0.2, name='dropout')(hidden)\n",
    "symptom_output = tf.keras.layers.Dense(10, name='symptom_output')(dropout)\n",
    "state_output = tf.keras.layers.Dense(4, activation='softmax', name='state_output')(symptom_output)\n",
    "\n",
    "one_head = tf.keras.Model(inputs=input, \n",
    "                          outputs=[symptom_output, state_output], \n",
    "                          name='one-head-model')\n",
    "\n",
    "one_head.summary()\n",
    "\n",
    "one_head.compile(optimizer='adam',\n",
    "                 loss=[tf.keras.losses.MeanAbsoluteError(),\n",
    "                       tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)],\n",
    "                 loss_weights=[0.5, 0.5],\n",
    "                 metrics=['mae', 'accuracy'])\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='state_output_accuracy', patience=3)\n",
    "\n",
    "one_head.fit(X_train_scaled, [y_train_symptoms, y_train_states_encoded], epochs=15, \n",
    "             validation_data=(X_test_scaled, [y_test_symptoms, y_test_states_encoded]),\n",
    "             callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-04T23:43:59.449298500Z"
    }
   },
   "outputs": [],
   "source": [
    "model_name='one_head'\n",
    "y_pred_states = one_head.predict(X_test_scaled)\n",
    "\n",
    "y_pred_states = np.argmax(y_pred_states[1][:], axis=1)\n",
    "\n",
    "cm_oh_class = confusion_matrix(y_test_states_encoded, y_pred_states)\n",
    "cr_oh_class = classification_report(y_test_states_encoded, y_pred_states)\n",
    "\n",
    "print(cm_oh_class)\n",
    "print(cr_oh_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.b) SHAP with states (4 classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-04T23:43:59.461301200Z"
    }
   },
   "outputs": [],
   "source": [
    "#prepare data for shap\n",
    "X_train_summary = shap.sample(X_train_scaled, 100)\n",
    "end = len(X_test_scaled)\n",
    "feature_names=X_test.columns \n",
    "classes_names_states= list(label_coding.keys())\n",
    "\n",
    "data_shap_df=pd.DataFrame(X_test_scaled[1:end:100,:], columns = feature_names)\n",
    "#data_shap_df.to_csv(shapvaluesStreamPath + \"/data_shap.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T23:43:59.470303Z",
     "start_time": "2024-09-04T23:43:59.466302900Z"
    }
   },
   "outputs": [],
   "source": [
    "def f_states(X):\n",
    "    return one_head.predict(X)[1]# with this function we select the second output of the model: vector of states \n",
    "\n",
    "explainer = shap.KernelExplainer(f_states, X_train_summary)  \n",
    "shap_values = explainer.shap_values(X_test_scaled[1:end:100, : ]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T23:43:59.560623900Z",
     "start_time": "2024-09-04T23:43:59.472303Z"
    }
   },
   "outputs": [],
   "source": [
    "#decoding values of BD stated to BD name\n",
    "classes_names_states = []\n",
    "for label in y_test_states_encoded:\n",
    "     classes_names_states.append(list(label_coding.keys())[list(label_coding.values()).index(label)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-04T23:43:59.487306Z"
    }
   },
   "outputs": [],
   "source": [
    "#I'm plotting the global explanation for all classes, varying the number of features to share\n",
    "model_name='one_head'\n",
    "for max_features in [20, 40, 86]: \n",
    "  shap.summary_plot(shap_values, X_test_scaled[1:end:100,:], plot_type=\"bar\", class_names= classes_names_states,\n",
    "                    feature_names = feature_names, max_display=max_features , show=False)\n",
    "  plt.gcf()\n",
    "  figname=graphsStreamPath+model_name+'_global_allclasses_states_'+str(max_features)+'.png'\n",
    "  plt.savefig(figname,dpi=150, bbox_inches='tight')\n",
    "  plt.plot()\n",
    "  plt.clf()\n",
    "\n",
    "#I'm plotting the global explanations for all the classes, varying the number of features to show\n",
    "#I'm iterating on the number of classes (numerical)\n",
    "for class_id in range(len(shap_values)):\n",
    "  #I'm iterating on the number of features I want to plot\n",
    "  for max_features in [20, 40, 86]: \n",
    "    shap.summary_plot(shap_values[class_id], X_test_scaled[1:end:100,:], feature_names = feature_names, max_display=max_features,show=False)\n",
    "    plt.gcf()\n",
    "    figname=graphsStreamPath+model_name+'_global_class'+str(class_id)+'_features'+ str(max_features)+'.png'\n",
    "    plt.savefig(figname,dpi=150, bbox_inches='tight')\n",
    "    plt.plot()\n",
    "    plt.clf()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-04T23:43:59.490830600Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "shap_values_0_class_oh = pd.DataFrame(shap_values[0], columns = feature_names)\n",
    "shap_values_1_class_oh = pd.DataFrame(shap_values[1], columns = feature_names)\n",
    "shap_values_2_class_oh = pd.DataFrame(shap_values[2], columns = feature_names)\n",
    "shap_values_3_class_oh = pd.DataFrame(shap_values[3], columns = feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.c) SHAP with symptoms (10 classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-04T23:43:59.494304700Z"
    }
   },
   "outputs": [],
   "source": [
    "#prepare data for shap\n",
    "X_train_summary = shap.sample(X_train_scaled, 100)\n",
    "end = len(X_test_scaled)\n",
    "feature_names=X_test.columns \n",
    "classes_names_states= list(label_coding.keys())\n",
    "\n",
    "data_shap_oh=pd.DataFrame(X_test_scaled[1:end:100,:], columns = feature_names)\n",
    "data_shap_oh.to_csv(shapvaluesStreamPath + \"/data_shap_onehead.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-04T23:43:59.496402900Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def f_symptoms(X):\n",
    "    return one_head.predict(X)[0]# with this function we select the second output of the model: vector of states \n",
    "\n",
    "explainer = shap.KernelExplainer(f_symptoms, X_train_summary)\n",
    "shap_values = explainer.shap_values(X_test_scaled[1:end:100,:]) \n",
    "classes_names=y_test_symptoms.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-04T23:43:59.499726300Z"
    }
   },
   "outputs": [],
   "source": [
    "max_features=20\n",
    "#I'm plotting the global summary for all the classes, only for 20 features\n",
    "shap.summary_plot(shap_values, X_test_scaled[1:end:100,:], plot_type=\"bar\", \n",
    "                  class_names= classes_names, max_display=max_features, feature_names = feature_names,show=False)\n",
    "plt.gcf()\n",
    "figname=graphsStreamPath+model_name+'_global_allclasses.png'\n",
    "plt.savefig(figname,dpi=150, bbox_inches='tight')\n",
    "plt.clf()\n",
    "\n",
    "for x in range(classes_names.shape[0]):\n",
    "  classes_names[x]\n",
    "  shap.summary_plot(shap_values[x], X_test_scaled[1:end:100,:], feature_names = feature_names,\n",
    "                    max_display=max_features,show=False) #you can change the maximum features to display \n",
    "  plt.gcf()\n",
    "  figname=graphsStreamPath+model_name+'_global_allclasses_'+classes_names[x]+'.png'\n",
    "  plt.savefig(figname,dpi=150, bbox_inches='tight')\n",
    "  plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-04T23:43:59.503357700Z"
    }
   },
   "outputs": [],
   "source": [
    "shap_values_0_symptom_oh = pd.DataFrame(shap_values[0], columns = feature_names)\n",
    "shap_values_1_symptom_oh = pd.DataFrame(shap_values[1], columns = feature_names)\n",
    "shap_values_2_symptom_oh = pd.DataFrame(shap_values[2], columns = feature_names)\n",
    "shap_values_3_symptom_oh = pd.DataFrame(shap_values[3], columns = feature_names)\n",
    "shap_values_4_symptom_oh = pd.DataFrame(shap_values[4], columns = feature_names)\n",
    "shap_values_5_symptom_oh = pd.DataFrame(shap_values[5], columns = feature_names)\n",
    "shap_values_6_symptom_oh = pd.DataFrame(shap_values[6], columns = feature_names)\n",
    "shap_values_7_symptom_oh = pd.DataFrame(shap_values[7], columns = feature_names)\n",
    "shap_values_8_symptom_oh = pd.DataFrame(shap_values[8], columns = feature_names)\n",
    "shap_values_9_symptom_oh = pd.DataFrame(shap_values[9], columns = feature_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "07bc74f91d1f8eb169c55d10374fd0fd42e378e12fcc798a378379f252fa0102"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
